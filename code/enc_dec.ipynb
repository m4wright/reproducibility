{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of machine_reading.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "rhC6Be9J8V3U",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import json\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "import math\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2kEAaGQBUkz0",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Download the required data\n",
        "\n",
        "\n",
        "\n",
        "import requests\n",
        "\n",
        "def downloadFile(url):\n",
        "    return requests.get(url).content\n",
        "      \n",
        "def loadTurkishValid():\n",
        "    sample_data_id = \"https://storage.googleapis.com/vincep/turkish_json/sample2.json\"\n",
        "    valid_sample_data = [json.loads(line) for line in downloadFile(sample_data_id).decode('utf-8').strip().splitlines()]\n",
        "    return valid_sample_data\n",
        "\n",
        "def loadTurkishVocab():\n",
        "    id = \"https://storage.googleapis.com/vincep/turkish_json/document.vocab\"\n",
        "   \n",
        "    vocab_data = downloadFile(id).decode('utf-8').strip().splitlines()\n",
        "    \n",
        "    vocab = []\n",
        "    for elem in vocab_data[2:]:\n",
        "        row = elem.split('\\t')\n",
        "        vocab.append({'index': int(row[0]), 'word': row[1].strip(), 'count': int(row[2])})\n",
        "    return vocab"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UpD6Fu_DgHXW",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Prepare the data for the word model\n",
        "\n",
        "def prepareData(data_list, seperator=1000000, max_document_length=60):\n",
        "    return [data[\"document_sequence\"][:max_document_length] + [seperator] + data[\"question_sequence\"] for data in data_list], [data[\"answer_sequence\"] for data in data_list]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5Rg4Gfbd0EHC",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Prepare the data for the byte model\n",
        "\n",
        "def processEntry(data, byte_limit=float(\"inf\"), word_seperator=ord(' ')):\n",
        "    entry = []\n",
        "    for word in data:\n",
        "        entry += list(word.encode('utf-8'))\n",
        "        entry.append(word_seperator)\n",
        "\n",
        "        if len(entry) >= byte_limit:\n",
        "            break\n",
        "      \n",
        "    return entry\n",
        "\n",
        "\n",
        "\n",
        "def processByByte(data_list, word_seperator=256, document_query_seperator=0, max_document_byte_length=400, max_question_byte_length=50):\n",
        "    processed_data = []\n",
        "    processed_answers = []\n",
        "    maxCount = 3\n",
        "        for data in data_list:\n",
        "            document_entry = processEntry(data[\"string_sequence\"], max_document_byte_length)\n",
        "            question_entry = processEntry(data[\"question_string_sequence\"], max_question_byte_length)\n",
        "\n",
        "            processed_data.append(document_entry + question_entry)\n",
        "\n",
        "            answer_entry = processEntry(data[\"raw_answers\"][:1])\n",
        "            processed_answers.append(answer_entry)\n",
        "    \n",
        "    return processed_data, processed_answers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Aq1EJ10IVGYI",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Load the vocabulary and the data\n",
        "\n",
        "vocab = loadTurkishVocab()\n",
        "valid_data = loadTurkishValid()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "19bdaseNJpRO",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# The Encoder\n",
        "\n",
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, embed_size, hidden_size, cell_type,\n",
        "                 bidirectional=False, num_layers=1):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.embed_size = embed_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.bidirectional = bidirectional\n",
        "        self.embedding = nn.Embedding(input_size,embed_size)\n",
        "        \n",
        "        if cell_type == 'lstm':\n",
        "            self.rnn = nn.LSTM(\n",
        "                embed_size, hidden_size, num_layers, bidirectional=self.bidirectional\n",
        "            )\n",
        "        elif cell_type == 'gru':\n",
        "            self.rnn = nn.GRU(\n",
        "                embed_size, hidden_size, num_layers, bidirectional=self.bidirectional\n",
        "            )\n",
        "        else:\n",
        "            raise ValueError('RNN cell type not valid')\n",
        "            \n",
        "            \n",
        "    def forward(self, inputs, lengths, hidden=None):\n",
        "        embedded = self.embedding(inputs)\n",
        "        packed = pack_padded_sequence(embedded, lengths)\n",
        "        outputs, hidden = self.rnn(packed, hidden)\n",
        "        outputs, output_lengths = pad_packed_sequence(outputs)  \n",
        "        if self.bidirectional:\n",
        "            outputs = outputs[:, :, :self.hidden_size] + outputs[:, : ,self.hidden_size:]\n",
        "        return outputs, hidden\n",
        "\n",
        "\n",
        "# The Attention Layer used in the decoder\n",
        "\n",
        "class Attn(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(Attn, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.attn = nn.Linear(self.hidden_size * 2, hidden_size)\n",
        "        self.v = nn.Parameter(torch.rand(hidden_size))\n",
        "        stdv = 1. / math.sqrt(self.v.size(0))\n",
        "        self.v.data.normal_(mean=0, std=stdv)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "        \n",
        "      \n",
        "\n",
        "    def forward(self, hidden, encoder_outputs):\n",
        "\n",
        "        max_len = encoder_outputs.size(0)\n",
        "        this_batch_size = encoder_outputs.size(1)\n",
        "        H = hidden.repeat(max_len,1,1).transpose(0,1)\n",
        "        encoder_outputs = encoder_outputs.transpose(0,1)\n",
        "        attn_energies = self.score(H,encoder_outputs) \n",
        "        return self.softmax(attn_energies).unsqueeze(1)\n",
        "\n",
        "    def score(self, hidden, encoder_outputs):\n",
        "        energy = F.tanh(self.attn(torch.cat([hidden, encoder_outputs], 2)))\n",
        "        energy = energy.transpose(2,1)\n",
        "        v = self.v.repeat(encoder_outputs.data.shape[0],1).unsqueeze(1)\n",
        "        energy = torch.bmm(v,energy)\n",
        "        return energy.squeeze(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2bdvCRrlgW6U",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# The Decoder class with the Attention layer\n",
        "\n",
        "class AttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, output_size, embed_size, hidden_size, cell_type, n_layers=1):\n",
        "        super(AttnDecoderRNN, self).__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embed_size = embed_size\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        self.embedding = nn.Embedding(output_size, embed_size)\n",
        "        self.attn = Attn(hidden_size)\n",
        "\n",
        "        if cell_type == 'lstm':\n",
        "            self.rnn = nn.LSTM(\n",
        "                embed_size + hidden_size, hidden_size, num_layers=n_layers\n",
        "            )\n",
        "        elif cell_type == 'gru':\n",
        "            self.rnn = nn.GRU(\n",
        "                embed_size + hidden_size, hidden_size, num_layers=n_layers\n",
        "            )\n",
        "        else:\n",
        "            raise ValueError('RNN cell type not valid')\n",
        "        \n",
        "        self.unembedding = nn.Linear(hidden_size, output_size)\n",
        "        self.log_softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, word_input, last_hidden, encoder_outputs):\n",
        "        batch_size = word_input.size(0)\n",
        "        \n",
        "        word_embedded = self.embedding(word_input).view(1, batch_size, -1)\n",
        "        \n",
        "        attn_weights = self.attn(last_hidden[-1], encoder_outputs)\n",
        "        context = attn_weights.bmm(encoder_outputs.transpose(0, 1))\n",
        "        context = context.transpose(0, 1)\n",
        "        \n",
        "        rnn_input = torch.cat((word_embedded, context), 2)\n",
        "        output, hidden = self.rnn(rnn_input, last_hidden)\n",
        "        output = output.squeeze(0)\n",
        "        output = self.unembedding(output)\n",
        "        output = self.log_softmax(output)\n",
        "        return output, hidden"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "M0xIN-GFJjzG",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Used to create the models. Different parameters passed to this class, and to its init_encoder and init_decoder methods,\n",
        "#     can be used to create all of the models used in this experiment\n",
        "\n",
        "class end2endRNN():\n",
        "    def __init__(self, batch_size, lr, max_pred_len=50):\n",
        "        super(end2endRNN, self).__init__()\n",
        "       \n",
        "        self.batch_size = batch_size\n",
        "        self.loss = nn.CrossEntropyLoss()\n",
        "        self.sos, self.eos, self.pad = 0, 1, 3\n",
        "        self.MAX_PRED_LEN = max_pred_len\n",
        "        self.lr = lr\n",
        "\n",
        "\n",
        "    def init_encoder(self, input_size, embed_size, hidden_size, **kwargs):\n",
        "        self.encoder = EncoderRNN(input_size, embed_size, hidden_size, **kwargs)\n",
        "        self.encoder_optimizer = optim.Adam(self.encoder.parameters(), lr=self.lr)\n",
        "\n",
        "    def init_decoder(self, output_size, embed_size, hidden_size, **kwargs):\n",
        "        self.decoder = AttnDecoderRNN(input_size, embed_size, hidden_size, **kwargs)\n",
        "        self.decoder_optimizer = optim.Adam(self.decoder.parameters(), lr=self.lr)\n",
        "        \n",
        "    def pad_seq(self, seq, max_length):\n",
        "        # 3 is the number corresponding to the token <NONE>\n",
        "        seq += [self.pad for i in range(max_length - len(seq))]\n",
        "        return seq\n",
        "\n",
        "    def prepareBatch(self, data, wrap_token=False):\n",
        "        \n",
        "        _data = [[self.sos] + t + [self.eos] for t in data] if wrap_token else data\n",
        "        \n",
        "        data_seqs = sorted(_data, key=lambda p: len(p), reverse=True)\n",
        "        \n",
        "        data_lengths = [len(s) for s in data_seqs]\n",
        "        data_padded = [self.pad_seq(s, max(data_lengths)) for s in data_seqs]\n",
        "        \n",
        "        data_var = Variable(torch.LongTensor(data_padded)).transpose(0, 1)\n",
        "\n",
        "        return data_var, data_lengths\n",
        "       \n",
        "    def step(self, inputs, targets, hidden_layers):\n",
        "        self.encoder_optimizer.zero_grad()\n",
        "        self.decoder_optimizer.zero_grad()\n",
        "       \n",
        "      \n",
        "        # Prepare data for batching\n",
        "        # SOS and EOS tokens are added in `prepareBatch` to the targets\n",
        "        input_batch, input_batch_len = self.prepareBatch(inputs)\n",
        "        target_batch, target_batch_len = self.prepareBatch(targets, wrap_token=True)\n",
        "        \n",
        "        max_target_length = max(target_batch_len)\n",
        "        \n",
        "        # Prepare decoder's hidden state\n",
        "        encoder_out, hidden_state = self.encoder.forward(input_batch, input_batch_len, hidden_layers)\n",
        "      \n",
        "        # Actual training occurs here (Decoder)\n",
        "        total_loss = 0\n",
        "        \n",
        "        for i in range(max_target_length - 1):\n",
        "            out, hidden_state = self.decoder.forward(target_batch[i], hidden_state, encoder_out)\n",
        "            current_loss = self.loss(out, target_batch[i+1])\n",
        "            total_loss += current_loss\n",
        "\n",
        "        total_loss.backward()\n",
        "\n",
        "        self.decoder_optimizer.step()\n",
        "        self.encoder_optimizer.step()\n",
        "\n",
        "        return total_loss.data[0], hidden_layers\n",
        "      \n",
        "      \n",
        "    def train(self, inputs, targets, batch_size, num_steps):\n",
        "        losses = []\n",
        "        hidden_layers = None\n",
        "        for step in range(num_steps):\n",
        "            loss_per_step = []\n",
        "            for data_index in range(0, len(inputs)-batch_size+1, batch_size):\n",
        "                batch_inputs = inputs[data_index: data_index+batch_size]\n",
        "                batch_targets = targets[data_index: data_index+batch_size]\n",
        "                loss, hidden_layers = self.step(batch_inputs, batch_targets, hidden_layers)\n",
        "                print(\"loss of {} at step {} and minibatch {}\".format(loss, step+1, data_index//batch_size + 1))\n",
        "                loss_per_step.append(loss)\n",
        "          \n",
        "            losses.append(loss_per_step)\n",
        "          \n",
        "          \n",
        "        return losses\n",
        "      \n",
        "     \n",
        "    def crossentropy_loss_batch(self, inputs, targets, hidden_layers):\n",
        "        # Prepare data for batching\n",
        "        # SOS and EOS tokens are added in prepareBatch to the targets\n",
        "        input_batch, input_batch_len = self.prepareBatch(inputs)\n",
        "        target_batch, target_batch_len = self.prepareBatch(targets, wrap_token=True)\n",
        "\n",
        "        max_target_length = max(target_batch_len)\n",
        "\n",
        "        # Prepare decoder's hidden state\n",
        "        encoder_out, hidden_state = self.encoder.forward(input_batch, input_batch_len, hidden_layers)\n",
        "\n",
        "        # Actual training occurs here (Decoder)\n",
        "        total_loss = 0\n",
        "       \n",
        "        for i in range(max_target_length - 1):\n",
        "            out, hidden_state = self.decoder.forward(target_batch[i], hidden_state, encoder_out)\n",
        "            current_loss = self.loss(out, target_batch[i+1])\n",
        "            total_loss += current_loss\n",
        "\n",
        "        return total_loss.data[0]\n",
        "   \n",
        "   \n",
        "    def crossentropy_loss(self, inputs, targets, batch_size, hidden_layers=None):\n",
        "        losses = []\n",
        "\n",
        "        for data_index in range(0, len(inputs)-batch_size+1, batch_size):\n",
        "            batch_inputs = inputs[data_index: data_index+batch_size]\n",
        "            batch_targets = targets[data_index: data_index+batch_size]\n",
        "            loss = end2endRNN.crossentropy_loss_batch(self,batch_inputs, batch_targets, hidden_layers)\n",
        "            losses.append(loss)\n",
        "           \n",
        "        return losses\n",
        "            \n",
        "        \n",
        "        \n",
        "        \n",
        "\n",
        "    def eval(self, inputs):\n",
        "        # Prepare data for batching\n",
        "        input_batch, input_batch_len = self.prepareBatch(inputs)\n",
        "        \n",
        "        # Prepare vectors for decoder\n",
        "        encoder_out, hidden_state = self.encoder(input_batch, input_batch_len)\n",
        "        \n",
        "        sentence = []\n",
        "        \n",
        "        x_t = Variable(torch.LongTensor([self.sos] * self.batch_size))\n",
        "        eos = Variable(torch.LongTensor([self.eos] * self.batch_size))\n",
        "        \n",
        "        \n",
        "        # Stop the infinite loop...\n",
        "        counter = 0\n",
        "        \n",
        "        # Predict\n",
        "        while counter < self.MAX_PRED_LEN and not x_t.equal(eos):\n",
        "            counter = counter + 1\n",
        "            output, hidden_state = self.decoder.forward(x_t, hidden_state, encoder_out)\n",
        "            val, ind = torch.max(output.data, 1)\n",
        "            x_t = Variable(ind)\n",
        "            sentence.append(ind)\n",
        "\n",
        "        return sentence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MTA5Ke7UMZNu",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# A function that takes all of the model hyper parameters, creates the model and trains it\n",
        "\n",
        "def train(lr, embedding_size, hidden_size, num_layers, cell_type, train_data, train_answer, batch_size, num_epochs, input_size, output_size,bidirectional):\n",
        "    rnn = end2endRNN(batch_size, lr)\n",
        "    rnn.init_encoder(input_size, embedding_size, hidden_size, cell_type=cell_type,bidirectional=bidirectional,num_layers=num_layers)\n",
        "    rnn.init_decoder(output_size, embedding_size, hidden_size, cell_type=cell_type)\n",
        "\n",
        "    start = time()\n",
        "    losses = rnn.train(train_data, train_answer, batch_size, num_epochs)\n",
        "    end = time()\n",
        "    return losses, (end-start), rnn"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}